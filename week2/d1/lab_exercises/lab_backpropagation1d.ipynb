{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# 1D Backpropagation Lab\n", "\n", "Welcome to the backpropagation lab! By the end of this lab, you will have\n", "\n", "- Implemented forward and backward passes for linear and one-hidden layer neural network regression models with a squared loss\n", "- Encountered an instance of the so-called *vanishing gradient* phenomenon\n", "\n", "Let's get started!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Instructions\n", "\n", "Throughout this lab, you will be implementing forward and backward passes (i.e. backpropagation) for computational graphs (i.e. functions). To make your work easier, you will be using a particular variable naming convention, which consists of the rules\n", "\n", "- Use the exact variable names which are used in the computational graph during the forward pass\n", "- Use `d`$\\cdot = \\overset{\\longleftarrow}{\\nabla_\\cdot}$ in the backward pass\n", "\n", "For example, consider the function\n", "\n", "$$\n", "f(x, y) = \\max(3x, y)\n", "$$\n", "\n", "and its computational graph for the invocation $f(2, 4)$\n", "\n", "![Forward Backward Example](images/Forward%20Backward%20Example.png)\n", "Using the variable naming convention above, the forward pass is computed as"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["x, y = 2, 4\n", "\n", "z = x * 3\n", "w = max(z, y)\n", "\n", "x, y, z, w"]}, {"cell_type": "markdown", "metadata": {}, "source": ["and the backward pass is computed as"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["dw = 1\n", "dy = 0 * dw\n", "dz = 1 * dw\n", "dx = 3 * dz\n", "\n", "dx, dz, dy, dw"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This variable naming convention will guide your thinking and allow you to focus on the algorithm itself.\n", "\n", "---\n", "\n", "# Linear Regression\n", "\n", "A linear regression model takes the form\n", "\n", "$$\n", "f(x, w, b) = wx + b.\n", "$$\n", "\n", "for a data point $x$ and parameters $w$ and $b$. The least-squares loss function takes the form\n", "\n", "$$\n", "\\mathcal{L}(\\hat{y}, y) = (\\hat{y} - y)^2.\n", "$$\n", "\n", "for predicted $\\hat{y}$ and true target $y$. Applying $L$ to $f$ yields the overall loss function\n", "\n", "\\begin{align*}\n", "L_\\text{LR}(x, y, w, b) &= \\mathcal{L}(f(x, w, b), y) \\\\\n", "                        &= [f(w, x, b) - y]^2 \\\\\n", "                        &= [(wx + b) - y]^2\n", "\\end{align*}\n", "\n", "for a given $(x, y)$ training pair and parameters $w$ and $b$."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Forward Pass\n", "\n", "### Tasks\n", "\n", "- Compute the forward pass for $L_\\text{LR}(3, 10, 2, 1)$ by hand on the computational graph $\\mathcal{G}$ = \n", "\n", "![Linear Regression Forward](images/linear%20regression%20forward.png)\n", "and fill in table\n", "\n", "| Variable | Value |\n", "|:--------:|:-----:|\n", "| $z$ | ? |\n", "| $\\hat{y}$ | ? |\n", "| $r$ | ? |\n", "| $\\ell$ | ? |\n", "\n", "- **Verify your answers with the instructor before continuing**\n", "- Write a computer program to compute the forward pass for $L_\\text{LR}(3, 10, 2, 1)$ on $\\mathcal{G}$\n", "- Verify your forward pass is correct by comparing your two sets of values\n", "\n", "### Requirements\n", "\n", "- Use the exact variable names as used in $\\mathcal{G}$\n", "- Implement the forward pass on $\\mathcal{G}$ exactly as shown and do not skip steps (e.g. merging multiple boxes into one operation)\n", "    - Use the variable name `y_hat` for $\\hat{y}$ "]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Backward Pass\n", "\n", "### Tasks\n", "\n", "- Compute gradients for all intermediate values in $\\mathcal{G}$ by hand with the values of $x$, $w$, $b$, and $y$ listed above on $\\mathcal{G}$ =\n", " \n", "![Linear Regression Backward](images/linear%20regression%20backward.png) \n", " and fill the following table\n", " \n", "| Variable | Value |\n", "|:--------:|:-----:|\n", "| $\\nabla_\\ell$ | ? |\n", "| $\\nabla_r$ | ? |\n", "| $\\nabla_{\\hat{y}}$ | ? |\n", "| $\\nabla_b$ | ? |\n", "| $\\nabla_z$ | ? |\n", "| $\\nabla_w$ | ? |\n", "\n", "- **Verify your answer with the instructor before continuing**\n", "- Compute gradients for all intermediate values in $\\mathcal{G}$ with code for the values of $x$, $y$, $w$, $b$ listed above\n", "- Verify the correctness of your code by comparing against the correct gradients\n", "\n", "### Hints\n", "\n", "- $\\overset{\\longleftarrow}{\\nabla_\\ell}$ = 1 will get you started\n", "- Reference the intermediate variables you computed above in the forward pass for computing all local gradients\n", "- Recall the rule for chaining $\\overset{\\longleftarrow}{\\nabla}_y$ to $\\overset{\\longleftarrow}{\\nabla}_x$ when $y = f(x)$ is\n", "\n", "![Chain Rule](images/chain%20rule.png)\n", "### Questions\n", "\n", "1. Before computing the backward pass by hand, what do you think the sign on $\\nabla{w}$ and $\\nabla{b}$ should be? How about $\\nabla_z$, $\\nabla_{\\hat{y}}$, and $\\nabla_r$? Justify your answer with intuition and not an equation."]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# One-Hidden-Layer Perceptron\n", "\n", "A one-hidden-layer perceptron model takes the form\n", "\n", "$$\n", "g(x, w_1, b_1, w_2, b_2) = \\max(w_1 x + b_1, 0)w_2 + b_2\n", "$$\n", "\n", "for a data point $x$ and parameters $w_1$, $b_1$, $w_2$, $b_2$. As defined above, the least-squares loss function takes the form\n", "\n", "$$\n", "\\mathcal{L}(\\hat{y}, y) = (\\hat{y} - y)^2.\n", "$$\n", "\n", "for predicted $\\hat{y}$ and true target $y$. Applying $L$ to $g$ yields the loss function\n", "\n", "\\begin{align*}\n", "L_\\text{MLP}(x, y, w_1, b_1, w_2, b_2)\n", "&= \\mathcal{L}(g(x, w_1, b_1, w_2, b_2), y) \\\\\n", "&= [g(x, w_1, b_1, w_2, b_2) - y]^2 \\\\\n", "&= [(\\max(w_1 x + b_1, 0)w_2 + b_2) - y]^2\n", "\\end{align*}\n", "\n", "for a given $(x, y)$ training pair and parameters $w_1$, $b_1$, $w_2$, $b_2$.\n", "\n", "## Forward Pass\n", "\n", "### Tasks\n", "\n", "- Compute the forward pass for $L_\\text{MLP}(2, 1, -1, 1, -2, 1.5)$ by hand the computational graph $\\mathcal{G}$ = \n", "\n", "![Multi-Layer Perceptron Forward](images/mlp%20forward.png)\n", "and fill in the table\n", "\n", "| Variable | Value |\n", "|:--------:|:-----:|\n", "| $z_1$ | ? |\n", "| $a$ | ? |\n", "| $h$ | ? |\n", "| $z_2$ | ? |\n", "| $\\hat{y}$ | ? |\n", "| $r$ | ? |\n", "| $\\ell$ | ? |\n", "\n", "- **Verify correctness of gradients with instructor before preceding**\n", "- Compute the forward pass for $L_\\text{MLP}(2, 1, -1, 1, -2, 1.5)$ via code\n", "- Verify the correctness of your implementation by comparing your two sets of values\n", "\n", "### Requirements\n", "\n", "- Use the exact variable names as used in the computational graph\n", "- Implement the computational graph exactly as shown and do not skip steps (e.g. merging multiple boxes into one operation)\n", "\n", "### Hints\n", "\n", "- Your your linear regression forward pass code as a starting point"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Backward Pass\n", "\n", "### Tasks\n", "\n", "- Compute all gradients by hand on the computational graph $\\mathcal{G}$ = \n", "\n", "![Multi-Layer Perceptron Backward](images/mlp%20backward.png)\n", "and fill in the table\n", "\n", "| Variable | Value |\n", "|:--------:|:-----:|\n", "| $\\nabla_\\ell$ | ? |\n", "| $\\nabla_r$ | ? |\n", "| $\\nabla_{\\hat{y}}$ | ? |\n", "| $\\nabla_{b_2}$ | ? |\n", "| $\\nabla_{z_2}$ | ? |\n", "| $\\nabla_{w_2}$ | ? |\n", "| $\\nabla_{h}$ | ? |\n", "| $\\nabla_{a}$ | ? |\n", "| $\\nabla_{b_1}$ | ? |\n", "| $\\nabla_{z_1}$ | ? |\n", "| $\\nabla_{w_1}$ | ? |\n", "\n", "- **Check with the instructor to verify your proposed solution before continuing**\n", "- Compute backpropagation on $\\mathcal{G}$ with code\n", "- Verify your two sets of answers agree\n", "\n", "### Requirements\n", "\n", "- Use the variable naming convention `d`$\\cdot = \\overset{\\longleftarrow}{\\nabla_\\cdot}$ For example, $\\overset{\\longleftarrow}{\\nabla_r}$ gets the variable name `dr`.\n", "\n", "### Hints\n", "\n", "- Use your linear regression backpropagation code as a starting point\n", "- Reference the intermediate variables you computed above in the forward pass for computing all local gradients\n", "- $\\overset{\\longleftarrow}{\\nabla_\\ell}$ = 1 will get you started"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Questions\n", "\n", "- What is the magnitude of $\\nabla_{w_1}$ as compared with $\\nabla_{w_2}$? In your own words, what does this mean? Is there a similar relationship between $\\nabla_{b_1}$ and $\\nabla_{b_2}$? If so, why?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- What is the function through which the gradient goes to zero? Why might this be a concern?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- Propose one possible solution for this problem."]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Bonus Tasks\n", "\n", "- Extend your one-hidden layer neural network to a three-hidden layer neural network model\n", "- Implement stochastic gradient descent to optimize your model on a single data point\n", "- Implement gradient checking to verify your backpropagation code"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.1"}}, "nbformat": 4, "nbformat_minor": 1}