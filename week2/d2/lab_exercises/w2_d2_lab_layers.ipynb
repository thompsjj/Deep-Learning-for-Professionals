{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Layers Lab\n",
    "\n",
    "Welcome to the Neural Networks Layers lab! By the end of this lab, you will have\n",
    "\n",
    "- Implemented Affine, ReLU, and Squared Loss layers\n",
    "- Built a modular implementation of a one-hidden layer perceptron model with a squared loss\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "---\n",
    "\n",
    "# Neural Network Layers\n",
    "\n",
    "A neural network *layer* is a unit of computation that knows how to\n",
    "\n",
    "- Compute a *forward* pass where it calculates its output(s) from its input(s)\n",
    "- Compute a *backward* pass where it calculates its input gradient(s) from its output gradient(s) \n",
    "\n",
    "A neural network *layer* can be implemented in many ways, one reasonable choice being a python class which conforms to the following interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def forward(self, inputs):\n",
    "        raise NotImplementedError('Forward pass not implemented!')\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        raise NotImplementedError('Backward pass not implemented!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "Let's start off simple with a Plus layer.\n",
    "\n",
    "![Plus Layer](images/Plus.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Plus(Layer):\n",
    "    def forward(self, a, b):\n",
    "        c = a + b\n",
    "        return c\n",
    "    \n",
    "    def backward(self, dc):\n",
    "        da, db = 1*dc, 1*dc\n",
    "        return da, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the Plus layer computes its output c given its inputs `a` and `b`. The Plus layer also computes its input gradients `da` and `db` given its output gradient `dc`.\n",
    "\n",
    "Notice that `Plus.backward()` does not need to know the value of `a` nor `b` (i.e. its inputs). Such a layer is called *stateless* because it doesn't need to remember anything from its forward pass.\n",
    "\n",
    "In contrast, some layers are *stateful*. A stateful layer requires knowledge of values that were computed during its forward pass in order to compute its backward pass. The Square layer is an example of a stateful layer.\n",
    "\n",
    "![Square Layer](images/Square.png)\n",
    "Notice the Square layer is stateful because $\\dfrac{\\partial y}{\\partial x}$ is a function of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Square(Layer):\n",
    "    def forward(self, x):\n",
    "        y = x**2\n",
    "        self.cache = locals()\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        x = self.cache['x']\n",
    "        dx = 2*x * dy\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retain knowledge of values computed during the forward pass of the Square layer, we call the python builtin `locals()` function right before exiting. The `locals()` function returns a `dict` containing all of the local variables in the current scope. It's basically a very convenient way to quickly record everything that's been computed in the forward pass. We save it to an attribute `self.cache` so that we can retrieve it in the backward pass.\n",
    "\n",
    "Once we have layers, we can chain them together to form a more interesting computational graphs. Here's an example of chaining together a `Plus` layer and a `Square` layer.\n",
    "\n",
    "![Pipeline](images/Pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 10, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plus, square = Plus(), Square()\n",
    "\n",
    "a, b = 3, 2\n",
    "\n",
    "c = plus.forward(a, b)\n",
    "y = square.forward(c)\n",
    "\n",
    "dy = 1\n",
    "dc = square.backward(dy)\n",
    "da, db = plus.backward(dc)\n",
    "\n",
    "da, db, dc, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, an invocation of a computational graph consists of four steps.\n",
    "\n",
    "1. Instantiate the layers of your computational graph\n",
    "2. Define the inputs values of the inputs to the graph\n",
    "3. Perfrom the forward pass\n",
    "4. Perform the backward pass (i.e. backpropagation)\n",
    "\n",
    "Enough examples. It's your turn to implement some layers!\n",
    "\n",
    "---\n",
    "\n",
    "# Affine Layer\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Implement an Affine layer which computes the function\n",
    "\n",
    "$$\n",
    "\\text{Affine}(x, w, b) = wx + b\n",
    "$$\n",
    "\n",
    "and hence corresponds to the computational graph\n",
    "\n",
    "![Affine Black Box](images/Affine%20Abstraction%20Black%20Box.png)\n",
    "### Requirements\n",
    "\n",
    "- Use the exact variable names as used in the computational graph\n",
    "- Use the variable naming convention `d`$\\cdot = \\overset{\\longleftarrow}{\\nabla_\\cdot}$ For example, $\\overset{\\longleftarrow}{\\nabla_r}$ gets the variable name `dr`.\n",
    "\n",
    "### Hints\n",
    "\n",
    "- Implement the Affine layer in terms of operations which have simple local gradients (i.e. are easy to backpropagate through) as in the computational graph\n",
    "\n",
    "![Affine White Box](images/Affine%20Abstraction%20White%20Box.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "- Why do you think we compute $\\nabla_x$? Recall in the previous lab, we only computed $\\nabla_w$ and $\\nabla_b$.\n",
    "\n",
    "### Answer\n",
    "\n",
    "- In the previous lab, $\\nabla_x$ was assumed to be our observed, fixed data. Hence it was no use to us compute $\\nabla_x$ because we could not change $x$. However, we can now have affine layers whose $x$ input refers to an activation from a previous layer. Hence it will likely be necessary to compute $\\nabla_x$ as part of chaining the gradient from the loss to parameters from earlier layers in the given architecture during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "- Implement a Rectified Linear Unit (ReLU) layer which computes the function\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(a) = \\begin{cases} 0 & \\text{if } a < 0 \\\\ a & \\text{otherwise} \\end{cases}\n",
    "$$\n",
    "\n",
    "and hence corresponds to the computational graph\n",
    "\n",
    "![ReLU Layer Black Box](images/ReLU%20Layer%20Black%20Box.png)\n",
    "### Requirements\n",
    "\n",
    "- Use the exact variable names as used in the computational graph\n",
    "- Use the variable naming convention `d`$\\cdot = \\overset{\\longleftarrow}{\\nabla_\\cdot}$ For example, $\\overset{\\longleftarrow}{\\nabla_r}$ gets the variable name `dr`.\n",
    "\n",
    "### Hints\n",
    "\n",
    "- Implement the ReLU layer in terms of operations which have simple local gradients (i.e. are easy to backpropagate through) as in the computational graph\n",
    "\n",
    "![ReLU Layer White Box](images/ReLU%20Layer%20White%20Box.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squared Loss Layer\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Implement a SquaredLoss layer which computes the function\n",
    "\n",
    "$$\n",
    "\\text{SquaredLoss}(\\hat{y}, y) = (\\hat{y} - y)^2\n",
    "$$\n",
    "\n",
    "and hence corresponds to the computational graph\n",
    "\n",
    "![Squared Loss Black Box](images/Squared%20Loss%20Black%20Box.png)\n",
    "### Requirements\n",
    "\n",
    "- Use the exact variable names as used in the computational graph\n",
    "- Use the variable naming convention `d`$\\cdot = \\overset{\\longleftarrow}{\\nabla_\\cdot}$ For example, $\\overset{\\longleftarrow}{\\nabla_r}$ gets the variable name `dr`.\n",
    "\n",
    "### Hints\n",
    "\n",
    "- Implement the SquaredLoss layer in terms of operations which have simple local gradients (i.e. are easy to backpropagate through) as in the computational graph\n",
    "\n",
    "![Squared Loss White Box](images/Squared%20Loss%20White%20Box.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hidden-Layer Neural Network Pipeline\n",
    "\n",
    "Recall from the last lab a one-hidden-layer neural network model takes the form\n",
    "\n",
    "$$\n",
    "g(x, w_1, b_1, w_2, b_2) = \\max(\\max(w_1 x + b_1, 0)w_2 + b_2, 0).\n",
    "$$\n",
    "\n",
    "Rewriting $g$ in terms of the layers we have defined yields\n",
    "\n",
    "$$\n",
    "g(x, w_1, b_1, w_2, b_2) = \\text{Affine}(\\text{ReLU}(\\text{Affine}(x, w_1, b_1)), w_2, b_2).\n",
    "$$\n",
    "\n",
    "Applying a squared loss to $g$ yields the loss function\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(x, y, w_1, b_1, w_2, b_2)\n",
    "&= \\text{SquaredLoss}(\\text{Affine}(\\text{ReLU}(\\text{Affine}(x, w_1, b_1)), w_2, b_2), y)\n",
    "\\end{align*}\n",
    "\n",
    "for a given $(x, y)$ training pair and parameters $(w_1, b_1, w_2, b_2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Compute $\\mathcal{L}(2, 1, -1, 1, -2, 1.5)$ as corresponding to the computational graph\n",
    "\n",
    "![MLP Layers Forward Numeric](images/MLP%20Layers%20Numeric%20Forward.png)\n",
    "### Requirements\n",
    "\n",
    "- Use only the layers you have defined in this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, 0, 1.5, 0.25)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Compute $\\nabla_{w_1}$, $\\nabla_{b_1}$, $\\nabla_{w_2}$, and $\\nabla_{b_2}$ as corresponding to the computational graph\n",
    "\n",
    "![MLP Layers Backward Numeric](images/MLP%20Layers%20Numeric%20Backward.png)\n",
    "### Requirements\n",
    "\n",
    "- Use the variable naming convention `d`$\\cdot = \\overset{\\longleftarrow}{\\nabla_\\cdot}$ For example, $\\overset{\\longleftarrow}{\\nabla_r}$ gets the variable name `dr`.\n",
    "\n",
    "### Hints\n",
    "\n",
    "- $\\overset{\\longleftarrow}{\\nabla_\\ell}$ = 1 will get you started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, -0.0, -0.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "- Did you need to make a call to `locals()` to cache anything during the neural network forward pass pipeline? Why or why not?\n",
    "\n",
    "### Answer\n",
    "\n",
    "- No because all of the caching is taken care of by the layers that we are hooking together.\n",
    "\n",
    "### Bonus Tasks\n",
    "\n",
    "- Implement a Sigmoid layer\n",
    "- Implement a Softamx layer\n",
    "- Implement a hinge loss layer\n",
    "- Implement a vectorized Affine layer\n",
    "- Implement a vectorized ReLU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
