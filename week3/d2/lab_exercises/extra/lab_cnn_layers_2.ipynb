{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Learning Objectives\n",
    "\n",
    "By the end of this lab, you will have\n",
    "\n",
    "- Implemented the forward and backward pass for a Dot layer\n",
    "- Gained an intuition for convolutional filters and the various types of actions they can perform\n",
    "- Implemented the forward and backward pass for a Convolutional layer\n",
    "- Verified the correctness of your implementations with gradient checking\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "# Dot Layers\n",
    "\n",
    "As a building block for implementing a convolutional layer, let's implement a Dot layer. A Dot layer takes as input two tensors and computes an elementwise product, followed by a total sum. It is identical to an inner product between two vectors, however both $\\mathbf{x}$ and $\\mathbf{w}$ are two-dimensional.\n",
    "\n",
    "Here is the Dot layer in computational graph form\n",
    "\n",
    "![Dot Layer Forwards](images/Dot%20Layer%20Forwards.png)\n",
    "as well as in algebraic form:\n",
    "\n",
    "$$\n",
    "\\text{Dot}(\\mathbf{x}, \\mathbf{w}) = \\sum_{i=1}^F \\sum_{j=1}^F \\mathbf{x}_{i,j} * \\mathbf{w}_{i,j}.\n",
    "$$\n",
    "\n",
    "## Implementing the Forward Pass\n",
    "\n",
    "Like all layers, we must first define the forward pass, which computes the output of our layer given the inputs.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Implement `dot_forward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def dot_forward(x, w):\n",
    "    \"\"\"Perform the forward pass on a dot product\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : a numpy 2darray\n",
    "    w : a numpy 2darray\n",
    "    \n",
    "    `x` and `w` are assumed to have the same shape. Think of `w` as\n",
    "    a convolutional filter and `x` a little region in the image that\n",
    "    is being convolved. Returns a float.\n",
    "    \n",
    "    \"\"\"\n",
    "    assert x.shape == w.shape\n",
    "    return np.sum(x * w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implementing the Backward Pass\n",
    "\n",
    "Now that we have defined the forward pass, all we have left is to define the backward pass. As a reminder, the backward pass takes the inputs and the gradient of the loss with respect to the output and uses the chain rule to compute the gradients of the input with respect to the loss.\n",
    "\n",
    "Before writing any code, it's very helpful to annotate the computational graph with the gradients.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- What is $\\nabla_\\mathbf{x}?$ Draw it on the computational graph.\n",
    "- What is $\\nabla_\\mathbf{w}?$ Draw it on the computational graph.\n",
    "- Implement `dot_backward()`\n",
    "\n",
    "### Hints\n",
    "\n",
    "- Use the computational graph to guide your implementation\n",
    "\n",
    "## Solution\n",
    "\n",
    "![Dot Layer Backwawrd](images/Dot%20Layer%20Backward.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def dot_backward(x, w, da=None):\n",
    "    \"\"\"Perform the backward pass on a dot layer\n",
    "    \n",
    "    Parameters\n",
    "    x : a numpy 2darray\n",
    "    w : a numpy 2darray\n",
    "    da : the gradient of the loss with respect to the output `a` of a Dot layer\n",
    "    \n",
    "    `x` and `w` are assumed to have the same shape. Think of `w` as\n",
    "    a convolutional filter and `x` a little region in the image that\n",
    "    is being convolved. Returns a dict of variable names to their\n",
    "    gradients (e.g. {'x': dx, 'w', dw})\n",
    "    \n",
    "    \"\"\"\n",
    "    assert x.shape == w.shape\n",
    "    da = 1 if da is None else da\n",
    "    return {'x': w, 'w': x}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Gradient-Checking Backward Pass\n",
    "\n",
    "When writing code to compute gradients analytically, [numerical gradient checking with finite differences](http://cs231n.github.io/optimization-1/#numerical) is an indispensible way of debugging your code.\n",
    "\n",
    "We've provided an api to perform gradient checking. To illustrate its use, consider the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lib.checking import gradient_check\n",
    "\n",
    "square = lambda x: x**2\n",
    "grad_square = lambda x: {'x': 2*x}\n",
    "x = np.array([2], dtype=np.float64)\n",
    "\n",
    "gradient_check(forward_f=square, backward_f=grad_square, x=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`gradient_check()` performs gradient checking on a layer and requires\n",
    "\n",
    "- A forward function `forward_f()`\n",
    "- A backward function `backward_f()`\n",
    "- A setting of the input(s) for checking the derivative.\n",
    "\n",
    "In the example provided, we have $f(x) = x^2$, $\\frac{\\partial f}{\\partial x}(x) = 2x$, and $x = 2$ for `forward_f()`, `backward_f()`, and `x`, respectively.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Gradient check `dot_backward()` with `lib.checking.gradient_check()` \n",
    "\n",
    "### Hints\n",
    "\n",
    "- You can use any `x` and `w` that conform to `dot_forward()`. I recommend `np.random.randn()`.\n",
    "- Use the example for a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(3, 3)\n",
    "w = np.random.randn(3, 3)\n",
    "\n",
    "gradient_check(forward_f=dot_forward, backward_f=dot_backward, x=x, w=w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Convolutional Layers\n",
    "\n",
    "### Implementing the Convolutional Forward Pass\n",
    "\n",
    "Now that we've defined a Dot layer, let's use it to define a convolutional layer. A convolutional layer can be seen as sequentially applying Dot layers to the entire input image.\n",
    "\n",
    "Let's implement the forward pass. As a reminder, a convolutional layer takes an image and a filter and *convolves* the filter with the image. A computational graph for a convolutional layer is depicted below:\n",
    "\n",
    "![Conv Layer Forward](images/Conv%20Layer%20Forward.png)\n",
    "For simplicity, you can assume the image only has a single channel and the filter is hard-coded to `3x3`.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Implement `conv_forward()`\n",
    "\n",
    "### Hints\n",
    "\n",
    "- Use `dot_forward()` in your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv_forward(X, w):\n",
    "    \"\"\"Perform a forward pass for a CNN layer\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy 2darray with shape (N, N)\n",
    "    w : numpy 2darray with shape (F, F)\n",
    "    \n",
    "    Assume that `w` is a 3x3 filter for simplicity.\n",
    "    Returns a 2D tensor `A` with shape (N-F+1, N-F+1).\n",
    "    \n",
    "    \"\"\"\n",
    "    N, N = X.shape\n",
    "    F, F = w.shape\n",
    "    assert (F, F) == (3, 3)\n",
    "    n = N-F + 1\n",
    "    \n",
    "    A = np.zeros([n, n])\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            i_x, j_x = i+1, j+1\n",
    "            i_start, i_end = i_x-1, i_x+1\n",
    "            j_start, j_end = j_x-1, j_x+1\n",
    "            \n",
    "            x = X[i_start:i_end+1, j_start:j_end+1]\n",
    "            A[i, j] = dot_forward(x, w)\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Forward Pass Verification\n",
    "\n",
    "Let's compare your forward pass implementation against a reference implementation of 2D convolution which we know to be correct - `scipy.signal.correlate2d()`.\n",
    "\n",
    "Why is it called `correlate2d()` and not `convole2d()`? Well, technically all along what we've been calling *convolution* is actually defined to be *correlation*, although the difference is not important.\n",
    "\n",
    "From this point forward, we'll be using an example image from `mnist`, an image of a `4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.linalg import norm\n",
    "from keras.datasets import mnist\n",
    "\n",
    "[X_mnist, y], _ = mnist.load_data()\n",
    "X = X_mnist[2].astype(np.float64)\n",
    "\n",
    "A = conv_forward(X, w)\n",
    "A_reference = signal.correlate2d(X, w, mode='valid')\n",
    "\n",
    "print('Conv check passed!' if norm(A-A_reference, ord='fro') < 1e-5 else 'Conv check failed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Uses for Convolution\n",
    "\n",
    "Convolutional has many uses, including being a tool for sharpening or blurring an image. To see how this can be accomplished, there are some special choices for filters who, after being convolved with an input image, produce an activation map which, when plotted, achieves this effect. Let's explore the various activation maps produced by a few different filters.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "Consider the filter\n",
    "$\n",
    "\\mathbf{w_1} =\n",
    "\\begin{bmatrix}\n",
    "    1 & 1 & 1 \\\\\n",
    "    0 & 0 & 0 \\\\\n",
    "    -1 & -1 & -1\n",
    "\\end{bmatrix}\n",
    "$.\n",
    "\n",
    "- What kind of image patch does the following filter respond most to?\n",
    "- Execute the following code cell and inspect the resulting activation map. Were you right? Are you surprised by the result? Qualitatively, what image features is $\\mathbf{w_1}$ looking for?\n",
    "\n",
    "### Hints\n",
    "\n",
    "- In the resulting activation map, black corresponds to low regions of excitement, grey to medium, and white to highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import array_to_img\n",
    "\n",
    "w = np.array([[ 1,  1,  1],\n",
    "              [ 0,  0,  0],\n",
    "              [-1, -1, -1]], dtype=np.float)\n",
    "\n",
    "img = array_to_img(np.expand_dims(X, axis=0), scale=False)\n",
    "display(img.resize(size=[128, 128]))\n",
    "\n",
    "A = signal.correlate2d(X, w, mode='valid')\n",
    "img = array_to_img(np.expand_dims(A, axis=0))\n",
    "img.resize(size=[128, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tasks\n",
    "\n",
    "Consider the filter\n",
    "$\n",
    "\\mathbf{w_2} =\n",
    "\\begin{bmatrix}\n",
    "    1 & 1 & 1 \\\\\n",
    "    1 & 1 & 1 \\\\\n",
    "    1 & 1 & 1\n",
    "\\end{bmatrix}.\n",
    "$\n",
    "\n",
    "- What kind of activation map will $\\mathbf{X} \\star \\mathbf{w_2}$ produce? Qualitatively, what will its appearance be?\n",
    "- Execute the following code cell and inspect the resulting activation map. Were you right? Are you surprised by the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import array_to_img\n",
    "\n",
    "w = np.array([[ 1,  1,  1],\n",
    "              [ 1,  1,  1],\n",
    "              [ 1,  1,  1]])\n",
    "\n",
    "img = array_to_img(np.expand_dims(X, axis=0), scale=False)\n",
    "display(img.resize(size=[128, 128]))\n",
    "\n",
    "A = signal.correlate2d(X, w, mode='valid')\n",
    "img = array_to_img(np.expand_dims(A, axis=0))\n",
    "img.resize(size=[128, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tasks\n",
    "\n",
    "Consider the filter\n",
    "$\n",
    "\\mathbf{w_3} =\n",
    "\\begin{bmatrix}\n",
    "    1 & 1 & 1 \\\\\n",
    "    1 & 1 & 1 \\\\\n",
    "    1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "$.\n",
    "\n",
    "- What will the resulting image look like we take $(\\mathbf{X} \\star \\mathbf{w_2}) + \\frac{1}{9} (\\mathbf{X} \\star \\mathbf{w_3}$)?\n",
    "- Execute the following code cell and inspect the resulting activation map. Were you right? Are you surprised by the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import array_to_img\n",
    "\n",
    "w = np.array([[ 0,  0,  0],\n",
    "              [ 0,  2,  0],\n",
    "              [ 0,  0,  0]], dtype=np.float) - (1/9)*np.array([[1, 1, 1],\n",
    "                                                               [1, 1, 1],\n",
    "                                                               [1, 1, 1]])\n",
    "\n",
    "img = array_to_img(np.expand_dims(X, axis=0), scale=False)\n",
    "display(img.resize(size=[128, 128]))\n",
    "\n",
    "A = signal.correlate2d(X, w, mode='valid')\n",
    "img = array_to_img(np.expand_dims(A, axis=0))\n",
    "img.resize(size=[128, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implementing the Backward Pass\n",
    "\n",
    "Now it's time to implement the backward pass. Once again, annotate the computational graph with the desired gradients. Note you do *not* have to solve for $\\frac{\\partial \\mathbf{A}}{\\partial \\mathbf{X}}$ nor $\\frac{\\partial \\mathbf{A}}{\\partial \\mathbf{w}}$ explicitly.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Annotate the computational graph with expressions for $\\nabla_\\mathbf{X}$ and $\\nabla_\\mathbf{w}$\n",
    "- Implement `conv_backward()`\n",
    "\n",
    "### Hints\n",
    "\n",
    "- Recall that a convolution operation is made up of repeated dots\n",
    "\n",
    "![Conv Hint](images/Conv%20Hint.png)\n",
    "- Recall the fact that when a parameter is used in multiple places in a computational graph, the chain rule tells us that its final gradient is the sum of its individual gradients\n",
    "\n",
    "![Split](images/Split.png)\n",
    "## Solution\n",
    "\n",
    "![Split](images/Conv%20Layer%20Backward.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv_backward(X, w, dA=None):\n",
    "    \"\"\"Perform a backward pass for a CNN layer\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy 2darray with shape (N, N)\n",
    "    w : numpy 2darray with shape (F, F)\n",
    "    dA : pre-padded numpy 2darray with shape (N-F+1, N-F+1)\n",
    "    \n",
    "    \"\"\"\n",
    "    N, N = X.shape\n",
    "    F, F = w.shape\n",
    "    dA = np.ones([N-F+1, N-F+1]) if dA is None else dA\n",
    "    n, n = dA.shape\n",
    "    assert n == N-F+1\n",
    "    \n",
    "    dX, dw = np.zeros_like(X), np.zeros_like(w)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            i_x, j_x = i+1, j+1            \n",
    "            i_start, i_end = i_x-1, i_x+1+1\n",
    "            j_start, j_end = j_x-1, j_x+1+1\n",
    "\n",
    "            x = X[i_start:i_end, j_start:j_end]\n",
    "            \n",
    "            dX[i_start:i_end, j_start:j_end] += dot_backward(x, w)['x'] * dA[i, j]\n",
    "            dw += dot_backward(x, w)['w']\n",
    "    \n",
    "    return {'X': dX, 'w': dw}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that you've implemented the forward and backward passes for a convolutional layer, ensure that you've implemented it correctly by performing gradient checking.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Perform gradient checking on `conv_backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from lib.checking import gradient_check\n",
    "\n",
    "gradient_check(forward_f=conv_forward, backward_f=conv_backward, X=X, w=w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
